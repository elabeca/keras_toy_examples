{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 3095,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 4579,\n",
       " 1005,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 1245,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# note that indices are offset by 3\n",
    "# indices 0, 1 and 2 are reserved for \"padding\", \"start of sequence\" and \"unknown\"\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing data (one-hot encoding the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape sequences:  (8982,)\n",
      "Shape results:  (8982, 10000)\n",
      "Shape sequences:  (2246,)\n",
      "Shape results:  (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    print(\"Shape sequences: \", sequences.shape)\n",
    "    print(\"Shape results: \", results.shape)\n",
    "\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for index in sequence:\n",
    "            results[i, index] = 1.\n",
    "    return results\n",
    "\n",
    "# our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing labels (one-hot encoding the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in way of doing one-hot encoding with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set aside some validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 10s - loss: 2.5304 - acc: 0.4964 - val_loss: 1.7173 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 7s - loss: 1.4425 - acc: 0.6883 - val_loss: 1.3436 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 7s - loss: 1.0929 - acc: 0.7657 - val_loss: 1.1702 - val_acc: 0.7440\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.8679 - acc: 0.8158 - val_loss: 1.0806 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 8s - loss: 0.7015 - acc: 0.8480 - val_loss: 0.9839 - val_acc: 0.7820\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.5657 - acc: 0.8792 - val_loss: 0.9406 - val_acc: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.4573 - acc: 0.9040 - val_loss: 0.9084 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.3690 - acc: 0.9232 - val_loss: 0.9347 - val_acc: 0.7890\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.3030 - acc: 0.9311 - val_loss: 0.8922 - val_acc: 0.8080\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 8s - loss: 0.2538 - acc: 0.9414 - val_loss: 0.9072 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 8s - loss: 0.2186 - acc: 0.9469 - val_loss: 0.9179 - val_acc: 0.8120\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 8s - loss: 0.1871 - acc: 0.9514 - val_loss: 0.9046 - val_acc: 0.8140\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1701 - acc: 0.9526 - val_loss: 0.9357 - val_acc: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1536 - acc: 0.9551 - val_loss: 0.9700 - val_acc: 0.8060\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1390 - acc: 0.9558 - val_loss: 0.9716 - val_acc: 0.8130\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 8s - loss: 0.1312 - acc: 0.9560 - val_loss: 1.0290 - val_acc: 0.8010\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1217 - acc: 0.9580 - val_loss: 1.0286 - val_acc: 0.7990\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1199 - acc: 0.9577 - val_loss: 1.0469 - val_acc: 0.8060\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1138 - acc: 0.9592 - val_loss: 1.1034 - val_acc: 0.7980\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 7s - loss: 0.1112 - acc: 0.9593 - val_loss: 1.0759 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06565bb080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoIkqfL/ASItOd331qsFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0656d56b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() # clear figure\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo')\n",
    "plt.plot(epochs, val_acc_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try 9 epochs as it's overfitting, but this time over the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/9\n",
      "8982/8982 [==============================] - 4s - loss: 2.5986 - acc: 0.5323 - val_loss: 1.7741 - val_acc: 0.6505\n",
      "Epoch 2/9\n",
      "8982/8982 [==============================] - 5s - loss: 1.4055 - acc: 0.7088 - val_loss: 1.3165 - val_acc: 0.7039\n",
      "Epoch 3/9\n",
      "8982/8982 [==============================] - 6s - loss: 1.0012 - acc: 0.7886 - val_loss: 1.1346 - val_acc: 0.7484\n",
      "Epoch 4/9\n",
      "8982/8982 [==============================] - 6s - loss: 0.7746 - acc: 0.8401 - val_loss: 1.0303 - val_acc: 0.7663\n",
      "Epoch 5/9\n",
      "8982/8982 [==============================] - 7s - loss: 0.6072 - acc: 0.8773 - val_loss: 0.9766 - val_acc: 0.7809\n",
      "Epoch 6/9\n",
      "8982/8982 [==============================] - 7s - loss: 0.4827 - acc: 0.9011 - val_loss: 0.9435 - val_acc: 0.7890\n",
      "Epoch 7/9\n",
      "8982/8982 [==============================] - 7s - loss: 0.3933 - acc: 0.9162 - val_loss: 0.9525 - val_acc: 0.7876\n",
      "Epoch 8/9\n",
      "8982/8982 [==============================] - 7s - loss: 0.3238 - acc: 0.9297 - val_loss: 0.9194 - val_acc: 0.7952\n",
      "Epoch 9/9\n",
      "8982/8982 [==============================] - 7s - loss: 0.2708 - acc: 0.9413 - val_loss: 0.9458 - val_acc: 0.7925\n",
      "2246/2246 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.94575634176775691, 0.79252003561887796]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,\n",
    "          one_hot_train_labels,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_test, one_hot_test_labels))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.79252003561887796, 'loss': 0.94575634176775691}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of a random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19723953695458593"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 46)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because it's softmax it's coefficient totals to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to handle / encode the labels and the loss is to cast them to an integer tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  1, ...,  3,  3, 24])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding the labels as integer arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use sparse categorical crossentropy loss instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/9\n",
      "8982/8982 [==============================] - 9s - loss: 0.1451 - acc: 0.9536 - val_loss: 1.2180 - val_acc: 0.7863\n",
      "Epoch 2/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1150 - acc: 0.9548 - val_loss: 1.2460 - val_acc: 0.7801\n",
      "Epoch 3/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1171 - acc: 0.9560 - val_loss: 1.2419 - val_acc: 0.7836\n",
      "Epoch 4/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1120 - acc: 0.9545 - val_loss: 1.2901 - val_acc: 0.7765\n",
      "Epoch 5/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1087 - acc: 0.9561 - val_loss: 1.2494 - val_acc: 0.7912\n",
      "Epoch 6/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1096 - acc: 0.9564 - val_loss: 1.3245 - val_acc: 0.7792\n",
      "Epoch 7/9\n",
      "8982/8982 [==============================] - 9s - loss: 0.1096 - acc: 0.9546 - val_loss: 1.2648 - val_acc: 0.7872\n",
      "Epoch 8/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1048 - acc: 0.9574 - val_loss: 1.3569 - val_acc: 0.7783\n",
      "Epoch 9/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.1069 - acc: 0.9531 - val_loss: 1.3535 - val_acc: 0.7818\n",
      "2080/2246 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_test, y_test))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.78183437224381325, 'loss': 1.3535090456662811}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 46)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions = model.predict(x_test)\n",
    "new_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(new_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of information bottle neck by reducing the number of hidden units to 8 instead of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/9\n",
      "8982/8982 [==============================] - 11s - loss: 3.3776 - acc: 0.0369 - val_loss: 2.8715 - val_acc: 0.2511\n",
      "Epoch 2/9\n",
      "8982/8982 [==============================] - 10s - loss: 2.3804 - acc: 0.5621 - val_loss: 2.0769 - val_acc: 0.6260\n",
      "Epoch 3/9\n",
      "8982/8982 [==============================] - 10s - loss: 1.6791 - acc: 0.6647 - val_loss: 1.6456 - val_acc: 0.6567\n",
      "Epoch 4/9\n",
      "8982/8982 [==============================] - 12s - loss: 1.3107 - acc: 0.6998 - val_loss: 1.4457 - val_acc: 0.6710\n",
      "Epoch 5/9\n",
      "8982/8982 [==============================] - 10s - loss: 1.1097 - acc: 0.7331 - val_loss: 1.3475 - val_acc: 0.6941\n",
      "Epoch 6/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.9716 - acc: 0.7671 - val_loss: 1.2736 - val_acc: 0.7186\n",
      "Epoch 7/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.8616 - acc: 0.7917 - val_loss: 1.2315 - val_acc: 0.7231\n",
      "Epoch 8/9\n",
      "8982/8982 [==============================] - 9s - loss: 0.7676 - acc: 0.8131 - val_loss: 1.2078 - val_acc: 0.7231\n",
      "Epoch 9/9\n",
      "8982/8982 [==============================] - 9s - loss: 0.6870 - acc: 0.8324 - val_loss: 1.1854 - val_acc: 0.7280\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "          one_hot_train_labels,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_test, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144/2246 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.72796081926073231, 'loss': 1.1853839009336775}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 128 unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/9\n",
      "8982/8982 [==============================] - 10s - loss: 2.1256 - acc: 0.5632 - val_loss: 1.4202 - val_acc: 0.6874\n",
      "Epoch 2/9\n",
      "8982/8982 [==============================] - 9s - loss: 1.0726 - acc: 0.7686 - val_loss: 1.1437 - val_acc: 0.7453\n",
      "Epoch 3/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.7419 - acc: 0.8440 - val_loss: 0.9899 - val_acc: 0.7752\n",
      "Epoch 4/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.5341 - acc: 0.8888 - val_loss: 0.9347 - val_acc: 0.7925\n",
      "Epoch 5/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.3837 - acc: 0.9220 - val_loss: 0.9432 - val_acc: 0.7916\n",
      "Epoch 6/9\n",
      "8982/8982 [==============================] - 14s - loss: 0.3034 - acc: 0.9344 - val_loss: 1.0227 - val_acc: 0.7738\n",
      "Epoch 7/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.2453 - acc: 0.9424 - val_loss: 0.9252 - val_acc: 0.8045\n",
      "Epoch 8/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.2109 - acc: 0.9475 - val_loss: 0.9575 - val_acc: 0.7988\n",
      "Epoch 9/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.1815 - acc: 0.9508 - val_loss: 1.0208 - val_acc: 0.7956\n",
      "2080/2246 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.79563668747088367, 'loss': 1.020824116463122}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                   one_hot_train_labels,\n",
    "                   epochs=9,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_test, one_hot_test_labels))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "dict(list(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 3 hidden layers of 128 hidden units each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/9\n",
      "8982/8982 [==============================] - 9s - loss: 2.8263 - acc: 0.3377 - val_loss: 1.9869 - val_acc: 0.3620\n",
      "Epoch 2/9\n",
      "8982/8982 [==============================] - 9s - loss: 1.5692 - acc: 0.3517 - val_loss: 1.4627 - val_acc: 0.3620\n",
      "Epoch 3/9\n",
      "8982/8982 [==============================] - 9s - loss: 1.2071 - acc: 0.3518 - val_loss: 1.3679 - val_acc: 0.3620\n",
      "Epoch 4/9\n",
      "8982/8982 [==============================] - 9s - loss: 0.9731 - acc: 0.4920 - val_loss: 1.0177 - val_acc: 0.7600\n",
      "Epoch 5/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.5372 - acc: 0.8715 - val_loss: 0.9939 - val_acc: 0.7832\n",
      "Epoch 6/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.4300 - acc: 0.8994 - val_loss: 0.9253 - val_acc: 0.8023\n",
      "Epoch 7/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.2769 - acc: 0.9370 - val_loss: 1.0226 - val_acc: 0.7921\n",
      "Epoch 8/9\n",
      "8982/8982 [==============================] - 10s - loss: 0.2930 - acc: 0.9228 - val_loss: 1.0190 - val_acc: 0.7974\n",
      "Epoch 9/9\n",
      "8982/8982 [==============================] - 11s - loss: 0.1967 - acc: 0.9456 - val_loss: 1.1318 - val_acc: 0.7801\n",
      "2080/2246 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.78005342837008429, 'loss': 1.1318295139981929}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    one_hot_train_labels,\n",
    "                    epochs=9,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_test, one_hot_test_labels))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "dict(list(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
